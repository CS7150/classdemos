{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "clJ_vNc8gCFO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mirror Classificiation Deep Network\n",
        "\n",
        "This notebook reproduces the experimental result in the seminal 1986\n",
        "paper by Rumelhart, Hinton, and Williams.\n",
        "\n",
        "The goal of this deep is to detect 6-pixel bit patterns that are symmetric.\n",
        "\n",
        "Here we create input data $X$ and output labels $y$ that contain all 64 of the cases:"
      ],
      "metadata": {
        "id": "mB7h_XhrULDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "\n",
        "##############################################################################\n",
        "# 1. Generate the 6-bit mirror-symmetry dataset\n",
        "##############################################################################\n",
        "def is_symmetric(bits):\n",
        "    return (bits[0] == bits[5]) and (bits[1] == bits[4]) and (bits[2] == bits[3])\n",
        "\n",
        "X_list = [list(np.binary_repr(i, width=6)) for i in range(64)]\n",
        "y_list = [[1] if is_symmetric(x) else [0] for x in X_list]\n",
        "\n",
        "X = np.array(X_list, dtype=float)\n",
        "y = np.array(y_list, dtype=float)\n",
        "\n",
        "# Display the dataset\n",
        "fig, axs = plt.subplots(8, 8, figsize=(8,4))\n",
        "for i, ax in enumerate(cell for row in axs for cell in row):\n",
        "    ax.imshow(X[i,None], cmap=\"Paired\", vmin=0, vmax=1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"y={int(y[i, 0])}\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tHM8Wx_DUuib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a two-layer neural network.\n",
        "\n",
        "Now we are going to define a two-layer perceptron network (MLP) with the neural connections needed to try to solve this problem.\n",
        "\n",
        "The $x_i$ nodes are inputs.  The $h_i$ nodes are interior \"hidden\" neurons, and the Out node is the visible \"output\" neuron.\n",
        "\n",
        "The weights leading into the $h_i$ nodes and the Out node are not determined ahead of time.  We will initialize them randomly and apply backpropagation to learn them."
      ],
      "metadata": {
        "id": "0ulGyJHTa43y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Untrained_MLP:\n",
        "    def __init__(self, n_input=6, n_hidden=2, n_output=1,\n",
        "                 init_range=0.3, seed=10):\n",
        "        \"\"\"\n",
        "        6->2->1 MLP:\n",
        "         - Sigmoid activation in hidden\n",
        "         - Sigmoid activation in output\n",
        "        \"\"\"\n",
        "        rng = np.random.RandomState(seed)\n",
        "\n",
        "        # Weight init in [-init_range, +init_range]\n",
        "        self.W1 = rng.uniform(-init_range, init_range, (n_input, n_hidden))\n",
        "        self.b1 = rng.uniform(-init_range, init_range, (n_hidden,))\n",
        "\n",
        "        self.W2 = rng.uniform(-init_range, init_range, (n_hidden, n_output))\n",
        "        self.b2 = rng.uniform(-init_range, init_range, (n_output,))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass. Returns (hidden_out, final_out).\n",
        "        \"\"\"\n",
        "        z1 = X @ self.W1 + self.b1\n",
        "        hidden_out = self.sigmoid(z1)\n",
        "        z2 = hidden_out @ self.W2 + self.b2\n",
        "        final_out = self.sigmoid(z2)\n",
        "        return hidden_out, final_out\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return 0/1 predictions by thresholding final_out>0.5.\"\"\"\n",
        "        _, final_out = self.forward(X)\n",
        "        return (final_out>0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "6kfu_5AfegDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define visualization functions\n",
        "\n",
        "The following defines `draw_mirror_network`, which draws the network and shows its current weights.  It also defines `plot_training_error` that we will use to plot the progress of training."
      ],
      "metadata": {
        "id": "clJ_vNc8gCFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_mirror_network(W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    Draw a diagram with circles, arrow lines, and weight labels on arrows.\n",
        "      - 6 input nodes horizontally at y=2, x=0..5\n",
        "      - 2 hidden nodes: top (h0) at (3,4), bottom (h1) at (3,0)\n",
        "      - 1 output node at (8,2)\n",
        "      - Circles bigger + arrow lines with weight labels on them.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12,5))\n",
        "    ax.set_xlim(-1, 14)\n",
        "    ax.set_ylim(-1, 7)\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Coordinates & circle sizes\n",
        "    input_positions = [(i * 2, 3) for i in range(6)]  # x=0..5, y=2\n",
        "    h0_pos = (9, 6)\n",
        "    h1_pos = (9, 0)\n",
        "    out_pos = (13, 3)\n",
        "\n",
        "    circle_radius = 0.6  # bigger circle\n",
        "\n",
        "    # 1) Draw input nodes\n",
        "    for i, (xx, yy) in enumerate(input_positions):\n",
        "        c = plt.Circle((xx, yy), radius=circle_radius, fill=False, lw=1.5)\n",
        "        ax.add_patch(c)\n",
        "        ax.text(xx, yy, f\"$x_{i}$\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    # 2) Hidden nodes\n",
        "    for idx, (hx, hy) in enumerate([h0_pos, h1_pos]):\n",
        "        c = plt.Circle((hx, hy), radius=circle_radius, fill=False, lw=1.5)\n",
        "        ax.add_patch(c)\n",
        "        ax.text(hx, hy, f\"$h_{idx}$\\nb={b1[idx]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "        # ax.text(hx, hy, f\"$h_{idx}$\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    # 3) Output node\n",
        "    ox, oy = out_pos\n",
        "    c = plt.Circle((ox, oy), radius=circle_radius, fill=False, lw=1.5)\n",
        "    ax.add_patch(c)\n",
        "    ax.text(ox, oy, f\"Out\\nb={b2[0]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "    # ax.text(ox, oy, f\"Out\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    # Helper arrow function\n",
        "    def draw_arrow_and_label(x0, y0, x1, y1, label, color=\"blue\"):\n",
        "\n",
        "        # Start and end arrow at circle edge, not center:\n",
        "        angle = np.arctan2(y1 - y0, x1 - x0)\n",
        "        xstart = x0 + circle_radius * np.cos(angle)\n",
        "        ystart = y0 + circle_radius * np.sin(angle)\n",
        "        xend = x1 - circle_radius * np.cos(angle)\n",
        "        yend = y1 - circle_radius * np.sin(angle)\n",
        "        ax.add_patch(FancyArrowPatch((xstart, ystart), (xend, yend),\n",
        "                                arrowstyle='-|>',\n",
        "                                mutation_scale=12,\n",
        "                                lw=1,\n",
        "                                color=color))\n",
        "        # Label at arrow midpoint\n",
        "        ax.text((x0 + x1) / 2, (y0 + y1) / 2, label, fontsize=9, color=color,\n",
        "                ha=\"center\", va=\"center\",\n",
        "                bbox=dict(boxstyle=\"square,pad=0.05\", fc=\"white\", ec=\"none\"))\n",
        "\n",
        "    # 4) Draw arrows for input->hidden\n",
        "    # W1 shape: (6,2)\n",
        "    for i, (xx, yy) in enumerate(input_positions):\n",
        "        w_h0 = W1[i, 0]\n",
        "        w_h1 = W1[i, 1]\n",
        "        draw_arrow_and_label(xx, yy, h0_pos[0], h0_pos[1], f\"{w_h0:.2f}\", \"blue\")\n",
        "        draw_arrow_and_label(xx, yy, h1_pos[0], h1_pos[1], f\"{w_h1:.2f}\", \"blue\")\n",
        "\n",
        "    # 5) Draw arrows for hidden->output\n",
        "    # W2 shape: (2,1)\n",
        "    draw_arrow_and_label(h0_pos[0], h0_pos[1], ox, oy, f\"{W2[0,0]:.2f}\", \"red\")\n",
        "    draw_arrow_and_label(h1_pos[0], h1_pos[1], ox, oy, f\"{W2[1,0]:.2f}\", \"red\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_error(history, label='MSE'):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(history, label=label)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Mean Squared Error\")\n",
        "    plt.title(\"Training (Sigmoid->Sigmoid + Momentum)\")\n",
        "    plt.grid(True)\n",
        "    plt.ylim(-0.01, max(history))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y13xYC_IgAPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the random untrained MLP\n",
        "\n",
        "Now let's create the MLP and test it on the data.\n",
        "\n",
        "When I run it, I get about 12% accuracy, which is what would happen if it says \"1\" all the time. If it says \"0\" all the time it would get about 87%.\n",
        "\n",
        "We'd hope to find a network that can do better than this."
      ],
      "metadata": {
        "id": "Pb6Cr5edg9bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = Untrained_MLP()\n",
        "preds = mlp.predict(X)\n",
        "acc = np.mean(preds==y)\n",
        "print(f\"Initial training accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "draw_mirror_network(mlp.W1, mlp.b1, mlp.W2, mlp.b2)"
      ],
      "metadata": {
        "id": "krPf-PzJg8S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define backpropagation functions\n",
        "\n",
        "To apply Rumelhart, Hinton, and Williams' idea, we need to calculate the error $e = \\sum_k (o_k - y_k)^2$, and then we also need to calculate the derivatives that lead to the error.\n",
        "\n",
        "$$g_i = \\frac{de}{dw_i} = \\sum_k  \\frac{\\partial e}{\\partial o_k} \\frac{\\partial o_k}{\\partial w_i} = \\sum_k (o_k - y_k) \\frac{\\partial o_k} {\\partial w_i} =  \\sum_k (o_k - y_k) \\sum_j  \\frac{\\partial o_k}{\\partial h_j} \\frac{\\partial h_j}{\\partial w_i}$$\n",
        "\n",
        "This derivative continues to expand out using the chain rule, we and we can compute it all by computing and multiplying the local partial derivatives $\\frac{\\partial o_k}{\\partial h_j}$ and $\\frac{\\partial h_j}{\\partial w_i}$ for every intermediate step $h_j$ within the network.\n",
        "\n",
        "Once we have the entire derivative $g_i$, then the paper recommends applying \"momentum\" to keep a running (decaying) sum of recent derivatives\n",
        "\n",
        "$$m_i \\leftarrow \\mu m_i + g_i$$\n",
        "\n",
        "And then finally we need to upate the parameters in the direction of this accumulated derivative\n",
        "\n",
        "$$w_i \\leftarrow w_i - \\lambda m_i$$\n",
        "\n",
        "The class `Trainable_MLP` adds a `backward` method that calculates the errors, all these derivatives and the momentum.  It then applies this information to make a change in the weights.\n",
        "\n",
        "A network is trained by applying this learning rule repeatedly, hundreds or thousands of times.\n"
      ],
      "metadata": {
        "id": "wCH8-tYKhLt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainable_MLP(Untrained_MLP):\n",
        "    def __init__(self, n_input=6, n_hidden=2, n_output=1,\n",
        "                 lr=0.1, momentum=0.9, init_range=0.3, seed=10):\n",
        "        super().__init__(n_input=n_input, n_hidden=n_hidden, n_output=n_output,\n",
        "                         init_range=init_range, seed=seed)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.W1_m = np.zeros_like(self.W1)\n",
        "        self.b1_m = np.zeros_like(self.b1)\n",
        "        self.W2_m = np.zeros_like(self.W2)\n",
        "        self.b2_m = np.zeros_like(self.b2)\n",
        "\n",
        "    def sigmoid_deriv(self, s):\n",
        "        # s is the sigmoid output\n",
        "        return s * (1.0 - s)\n",
        "\n",
        "    def backward(self, X, hidden_out, final_out, y):\n",
        "        \"\"\"\n",
        "        Backprop with momentum.\n",
        "        X: shape (batch, 6)\n",
        "        hidden_out: shape (batch, 2)\n",
        "        final_out: shape (batch, 1)\n",
        "        y: shape (batch, 1)\n",
        "        \"\"\"\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # Output delta\n",
        "        error = final_out - y  # shape (batch,1)\n",
        "        d_final = 2 * error * self.sigmoid_deriv(final_out)\n",
        "\n",
        "        # Hidden delta\n",
        "        d_hidden = (d_final @ self.W2.T) * self.sigmoid_deriv(hidden_out)\n",
        "\n",
        "        # Grad for W2,b2\n",
        "        grad_W2 = hidden_out.T @ d_final / batch_size   # (2,1)\n",
        "        grad_b2 = np.mean(d_final, axis=0)             # (1,)\n",
        "\n",
        "        # Grad for W1,b1\n",
        "        grad_W1 = X.T @ d_hidden / batch_size          # (6,2)\n",
        "        grad_b1 = np.mean(d_hidden, axis=0)            # (2,)\n",
        "\n",
        "        # Momentum update for W2,b2\n",
        "        self.W2_m = self.momentum*self.W2_m - self.lr*grad_W2\n",
        "        self.b2_m = self.momentum*self.b2_m - self.lr*grad_b2\n",
        "        self.W2 += self.W2_m\n",
        "        self.b2 += self.b2_m\n",
        "\n",
        "        # Momentum update for W1,b1\n",
        "        self.W1_m = self.momentum*self.W1_m - self.lr*grad_W1\n",
        "        self.b1_m = self.momentum*self.b1_m - self.lr*grad_b1\n",
        "        self.W1 += self.W1_m\n",
        "        self.b1 += self.b1_m\n",
        "\n",
        "    def train_on_batch(self, X, y):\n",
        "        \"\"\"One epoch of forward+backward over the full dataset.\"\"\"\n",
        "        hidden_out, final_out = self.forward(X)\n",
        "        self.backward(X, hidden_out, final_out, y)\n",
        "        mse = np.mean((final_out - y)**2)\n",
        "        return mse"
      ],
      "metadata": {
        "id": "rFTsv-Yxl6nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the weights of the MLP\n",
        "\n",
        "To train the weights, we repeatedly evaluate the error, calculate the derivatives, and update the parameters."
      ],
      "metadata": {
        "id": "Ht1xzP_Gmq05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = Trainable_MLP(n_input=6, n_hidden=2, n_output=1,\n",
        "                    lr=0.1,\n",
        "                    momentum=0.9,\n",
        "                    init_range=0.5,\n",
        "                    seed=10)\n",
        "\n",
        "epochs = 200\n",
        "mse_history = []\n",
        "for epoch in range(epochs):\n",
        "    mse = mlp.train_on_batch(X, y)\n",
        "    mse_history.append(mse)\n",
        "\n",
        "preds = mlp.predict(X)\n",
        "acc = np.mean(preds==y)\n",
        "print(f\"Final training accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "plot_training_error(mse_history)\n",
        "\n",
        "draw_mirror_network(mlp.W1, mlp.b1, mlp.W2, mlp.b2)\n"
      ],
      "metadata": {
        "id": "KqErRz-3I9id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn: improve training\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. What is the final accuracy of the mirror network?  Is it near 100\\%?\n",
        "2. Do the final neural network weights resemble the weights in the Rumelhart paper?\n",
        "3. Can you find a change in hyperparameters (learning rate, momentum or training epochs) that improves accuracy?\n",
        "4. Once the accuracy of the network is neat 100%, do the weights resemble Rumelhart?\n",
        "5. What is \"overfitting\"?  Is this model overfitting?"
      ],
      "metadata": {
        "id": "W_uOVdLBr858"
      }
    }
  ]
}