{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toeplitz Convolution in 1D and 2D: Padding, Shifts, and Parameter Counting\n",
    "\n",
    "In this notebook, we:\n",
    "1. Demonstrate 1D convolution with a **Toeplitz** matrix.\n",
    "2. Define and test **equivariance** over integer shifts.\n",
    "3. Show how **padding** changes the output size and affects equivariance.\n",
    "4. Move to a **2D** case (4×5 input), do an **`im2col`** explanation, and replicate the convolution via an explicitly filled `Linear` layer.\n",
    "5. Wrap up with an exercise on **parameter counts**:\n",
    "   - Full linear dimension,\n",
    "   - Nonzero Toeplitz entries,\n",
    "   - Actual number of kernel parameters.\n",
    "\n",
    "We also set `torch.set_grad_enabled(False)` to ensure no gradient overhead while we do these manual operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"torch.set_grad_enabled(False) is now active.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 1D Convolution (Length=10)\n",
    "\n",
    "We’ll do a **valid** 1D convolution:\n",
    "- Input length = **10** (mostly zeros at edges). \n",
    "- A 3-tap kernel (length=3). \n",
    "- **No bias** in the convolution.\n",
    "\n",
    "### 1.1 Creating and Filling the Conv1d Kernel\n",
    "Let’s define a small kernel, e.g. \\([1,2,1]\\)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Our 1D input: shape (batch=1, channels=1, length=10)\n",
    "x_1d = torch.tensor([[[0.,1.,2.,3.,4.,5.,6.,0.,0.,0.]]])\n",
    "print(\"x_1d shape:\", x_1d.shape)\n",
    "print(\"x_1d:\", x_1d)\n",
    "\n",
    "# Conv1d with kernel_size=3, out_channels=1, no bias.\n",
    "conv1d_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "\n",
    "# Manually set the kernel to [1,2,1]\n",
    "with torch.no_grad():\n",
    "    # We'll do something like conv1d_layer.weight[0,0] = torch.tensor([1.,2.,1.])\n",
    "    conv1d_layer.weight[0,0] = torch.tensor([1., 2., 1.])\n",
    "\n",
    "print(\"Conv1D kernel:\", conv1d_layer.weight)\n",
    "\n",
    "# Forward pass (valid convolution -> output length = 10-3+1 = 8)\n",
    "y_1d_conv = conv1d_layer(x_1d)\n",
    "print(\"Output shape:\", y_1d_conv.shape)\n",
    "print(\"Output:\", y_1d_conv)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Output length is } 10 - 3 + 1 = 8.$$\n",
    "\n",
    "### 1.2 Equivariance\n",
    "\n",
    "A function \\(f\\) is **equivariant** to a transformation \\(\\tau\\) if:\n",
    "$$\n",
    "f(\\tau(x)) \\;=\\; \\tau\\bigl(f(x)\\bigr).\n",
    "$$\n",
    "In the case of **shift** in 1D, \\(\\tau\\) might shift the signal by \\(n\\) steps. Convolution with “valid” boundaries is only partially shift-equivariant: if you shift too far, the kernel will have no data to convolve. Let’s define a quick `shift(n)` function and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def shift_1d(x, n):\n",
    "    \"\"\"\n",
    "    Shift the 1D tensor by n steps.\n",
    "    We'll keep the same length, so we lose data at one boundary\n",
    "    and add zeros at the other.\n",
    "    x: shape (batch=1, channel=1, length=L)\n",
    "    n>0 => shift right, n<0 => shift left.\n",
    "    \"\"\"\n",
    "    L = x.shape[-1]\n",
    "    shifted = torch.zeros_like(x)\n",
    "\n",
    "    if n >= 0:\n",
    "        # shift right by n.\n",
    "        # x[..., 0 : L-n] -> shifted[..., n : L]\n",
    "        src_slice = x[..., 0 : L-n]\n",
    "        dst_slice = shifted[..., n : L]\n",
    "        dst_slice[:] = src_slice\n",
    "    else:\n",
    "        # shift left by |n|\n",
    "        n_abs = -n\n",
    "        src_slice = x[..., n_abs : L]\n",
    "        dst_slice = shifted[..., 0 : L-n_abs]\n",
    "        dst_slice[:] = src_slice\n",
    "\n",
    "    return shifted\n",
    "\n",
    "# Quick test of shift_1d:\n",
    "print(\"Original x_1d:\", x_1d)\n",
    "x_1d_shifted2 = shift_1d(x_1d, 2)\n",
    "print(\"Shifted by 2:\", x_1d_shifted2)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a **range** of shifts (say \\(-5\\) to \\(+5\\)) and see if the convolution output is also shifted accordingly.\n",
    "But because we do **valid** convolution, we expect that large shifts will break the alignment.\n",
    "\n",
    "### 1.2.1 **Student Exercise**\n",
    "Fill in the code below to:\n",
    "1. Create `x_shifted = shift_1d(x_1d, n)`.\n",
    "2. Compute `y_shifted = conv1d_layer(x_shifted)`.\n",
    "3. Compare it to shifting the original `y_1d_conv` by `n` steps.\n",
    "4. Observe for which `n` you get `y_shifted ≈ shift_1d(y_1d_conv, n)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for n in range(-5, 6):\n",
    "    x_shifted = shift_1d(x_1d, n)\n",
    "    y_shifted = conv1d_layer(x_shifted)\n",
    "    # We'll shift the original y_1d_conv by n as well, but that is shape (1,1,8)\n",
    "    y_1d_shifted_ref = shift_1d(y_1d_conv, n)\n",
    "\n",
    "    # Compare numerically (just print or store difference). We only do a small snippet.\n",
    "    diff = (y_shifted - y_1d_shifted_ref).abs().sum()\n",
    "    print(f\"Shift n={n:2d}, difference={diff.item():.3f}\")\n",
    "\n",
    "print(\"\\nWhere is it roughly zero? Discuss your observations.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect small shifts (like \\(-1,0,1\\)) might show partial equivariance, but once the shift is big enough that the kernel can't align with the data region, it breaks.\n",
    "\n",
    "---\n",
    "### 1.3 Building an Explicit Toeplitz Matrix in `nn.Linear`\n",
    "\n",
    "The output length is 8, and the input length is 10. So a fully connected layer from **10** to **8** can replicate the same transform if we fill it with the Toeplitz pattern:\n",
    "$$\n",
    "\\text{weight} \\in \\mathbb{R}^{8 \\times 10}, \\quad y = \\text{weight} \\cdot x.\n",
    "$$\n",
    "We only need **3** unique weights \\((w_0, w_1, w_2)\\) repeated across the rows. \n",
    "\n",
    "**Student Exercise**: Fill in the matrix so that row 0 picks up \\([x_0, x_1, x_2]\\) with \\([w_0, w_1, w_2]\\), row 1 picks up \\([x_1, x_2, x_3]\\), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create the linear layer from 10->8, no bias.\n",
    "fc1d = nn.Linear(in_features=10, out_features=8, bias=False)\n",
    "\n",
    "# We'll manually fill it.\n",
    "with torch.no_grad():\n",
    "    fc1d.weight.zero_()\n",
    "\n",
    "# Let's see the shape:\n",
    "print(\"fc1d.weight.shape =\", fc1d.weight.shape)\n",
    "print(\"Initially:\", fc1d.weight)\n",
    "\n",
    "# STUDENT: fill the Toeplitz pattern. We know w=[1,2,1].\n",
    "# row i => columns i..i+2 get [1,2,1]. We'll do a simple loop.\n",
    "with torch.no_grad():\n",
    "    wvals = [1., 2., 1.]\n",
    "    for i in range(8):  # rows\n",
    "        fc1d.weight[i, i  ] = wvals[0]\n",
    "        fc1d.weight[i, i+1] = wvals[1]\n",
    "        fc1d.weight[i, i+2] = wvals[2]\n",
    "\n",
    "print(\"\\nAfter filling:\")\n",
    "print(fc1d.weight)\n",
    "\n",
    "# Compare with conv1d_layer results:\n",
    "x_1d_flat = x_1d.view(1,10)  # shape (1,10)\n",
    "y_fc1d = fc1d(x_1d_flat)\n",
    "print(\"\\nfc1d output shape:\", y_fc1d.shape)\n",
    "print(\"fc1d output:\", y_fc1d)\n",
    "\n",
    "diff_to_conv = y_fc1d.view(-1) - y_1d_conv.view(-1)\n",
    "print(\"\\nDifference vs conv:\", diff_to_conv)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, the difference should be near 0.\n",
    "\n",
    "---\n",
    "## Part 2: Adding Padding=1 in 1D\n",
    "\n",
    "Now let's see how **padding** affects the output shape and equivariance. If we do `padding=1` with a `Conv1d`, a kernel of length=3, and input length=10, the output length remains **10** (since the kernel can start at the left edge with a pad). We’ll see that large shifts might cause boundary issues.\n",
    "\n",
    "### 2.1 Padded Conv1d\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We'll re-create a new conv1d for the padded case.\n",
    "conv1d_pad = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
    "with torch.no_grad():\n",
    "    conv1d_pad.weight[0,0] = torch.tensor([1.,2.,1.])\n",
    "\n",
    "# This should produce an output of length=10.\n",
    "y_pad = conv1d_pad(x_1d)\n",
    "print(\"Padding=1 -> output shape:\", y_pad.shape)\n",
    "print(\"Output:\", y_pad)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Toeplitz Matrix for the Padded Case\n",
    "\n",
    "Now the output is length=10, and the input is length=10, so we can do a `Linear(10,10, bias=False)` and fill it with the Toeplitz pattern for a 3-tap kernel **plus** the notion of 1 padding on each side. But effectively, the kernel can index beyond the left or right edges if we didn't pad. We'll represent that as extra columns if you wish. However, in many references, the Toeplitz approach for padding is effectively a 10×10 with some of the top-left corners hosting the kernel partial.\n",
    "\n",
    "**Student**: Fill in the 10×10 so that it matches `padding=1` semantics. Or see below for the direct approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fc1d_pad = nn.Linear(10, 10, bias=False)\n",
    "with torch.no_grad():\n",
    "    fc1d_pad.weight.zero_()\n",
    "\n",
    "print(\"Initial fc1d_pad weights:\", fc1d_pad.weight)\n",
    "\n",
    "# We'll define a small function to fill in the padded topelitz.\n",
    "def fill_toeplitz_padded(linear_layer, wvals):\n",
    "    # length=10 in/out, kernel=3, padding=1 => each output i sees x[i-1], x[i], x[i+1], ignoring out-of-bounds.\n",
    "    with torch.no_grad():\n",
    "        linear_layer.weight.zero_()\n",
    "        for out_i in range(10):\n",
    "            # out_i indexes the output.\n",
    "            # The kernel is [w0, w1, w2], which correspond to x[out_i-1], x[out_i], x[out_i+1].\n",
    "            for k in range(3):\n",
    "                # position in the input = out_i + (k-1)\n",
    "                in_i = out_i + (k-1)\n",
    "                if in_i>=0 and in_i<10:\n",
    "                    linear_layer.weight[out_i, in_i] = wvals[k]\n",
    "\n",
    "fill_toeplitz_padded(fc1d_pad, [1.,2.,1.])\n",
    "print(\"\\nAfter filling:\")\n",
    "print(fc1d_pad.weight)\n",
    "\n",
    "# Compare with conv1d_pad\n",
    "y_fc1d_pad = fc1d_pad(x_1d.view(1,10))\n",
    "diff_pad = y_fc1d_pad.view(-1) - y_pad.view(-1)\n",
    "print(\"\\nDifference:\", diff_pad)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the difference is near zero, we’ve matched the padded convolution.\n",
    "\n",
    "### 2.3 Check Equivariance with Padding=1\n",
    "We can again try shifting the input in the range \\(-5..5\\). With padding, the output is always length=10. But does that make it fully shift-equivariant?\n",
    "\n",
    "In practice, the *artificial* zeros at the edges might still cause boundary effects that break perfect equivariance once the shift is large enough.\n",
    "\n",
    "> **Student**: Try `shift_1d(x_1d, n)`, pass to `conv1d_pad`, compare to shifting the *output* by `n`. Where does it break?\n",
    "\n",
    "---\n",
    "## Part 3: A 2D Example (4×5 input) and `im2col`\n",
    "\n",
    "We shrink the 2D input to shape 4×5. A 3×3 kernel with **valid** convolution yields output shape 2×3.\n",
    "\n",
    "### 3.1 The 2D Input\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img_2d = torch.tensor([\n",
    "    [1,1,0,1,0],\n",
    "    [0,1,1,1,1],\n",
    "    [1,0,0,1,0],\n",
    "    [1,1,1,0,0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "print(\"Image shape: 4x5\")\n",
    "print(img_2d)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.imshow(img_2d, cmap='gray')\n",
    "plt.title(\"Binary Image (4x5)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "x_2d = img_2d.unsqueeze(0).unsqueeze(0)  # (1,1,4,5)\n",
    "print(\"x_2d shape:\", x_2d.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define a 3×3 Kernel and Convolve\n",
    "The valid output shape is (4-3+1)×(5-3+1) = 2×3.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "with torch.no_grad():\n",
    "    # Let's define some pattern for the 3x3 kernel:\n",
    "    # e.g.\n",
    "    #  [ 1, 0, 1]\n",
    "    #  [-1, 2, 0]\n",
    "    #  [ 1, 1,-1]\n",
    "    conv2d.weight[0,0] = torch.tensor([\n",
    "        [ 1.,  0.,  1.],\n",
    "        [-1.,  2.,  0.],\n",
    "        [ 1.,  1., -1.]\n",
    "    ])\n",
    "\n",
    "y_2d = conv2d(x_2d)\n",
    "print(\"2D conv output shape:\", y_2d.shape)  # (1,1,2,3)\n",
    "print(\"Output:\", y_2d)\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.imshow(y_2d[0,0,:,:], cmap='viridis')\n",
    "plt.title(\"Conv2d Output (2x3)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `im2col` and Flattening Explanation\n",
    "\n",
    "If we flatten the **4×5** input in **row-major** order, we get 20 entries:\n",
    "$$\n",
    "\\text{vec}(X) = [\\,x_{0,0},\\; x_{0,1},\\;\\dots,\\; x_{0,4},\\; x_{1,0},\\dots, x_{3,4}\\,]^\\top \\in \\mathbb{R}^{20}.\n",
    "$$\n",
    "And the **2×3** output is 6 entries:\n",
    "$$\n",
    "\\text{vec}(Y) = [\\,y_{0,0},\\; y_{0,1},\\; y_{0,2},\\; y_{1,0},\\; y_{1,1},\\; y_{1,2}\\,]^\\top \\in \\mathbb{R}^{6}.\n",
    "$$\n",
    "The 3×3 kernel has 9 parameters, repeated in a **block-Toeplitz** pattern of size (6×20).\n",
    "\n",
    "**`im2col`** would produce a (9 × 6) matrix of patches. Each patch is 3×3 flattened to 9, stacked across 2×3 = 6 positions. Multiplying that by the 9 kernel parameters yields the same result.\n",
    "\n",
    "### 3.4 Exercise: Fill a `Linear(20,6)` with the Toeplitz Layout\n",
    "We create a `nn.Linear(20, 6, bias=False)`. Then for each output position `(r,c)`, we place the kernel into the correct 9 positions referencing `(r+dr, c+dc)` in the input. Students fill in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fc2d = nn.Linear(in_features=20, out_features=6, bias=False)\n",
    "with torch.no_grad():\n",
    "    fc2d.weight.zero_()\n",
    "\n",
    "print(\"fc2d weight shape:\", fc2d.weight.shape)  # (6,20)\n",
    "print(\"Initially:\", fc2d.weight)\n",
    "\n",
    "def fill_block_toeplitz_2d(linear_layer, kernel_3x3):\n",
    "    # kernel shape: (3,3)\n",
    "    # output shape: (2,3) => 6 positions\n",
    "    # input shape:  (4,5) => 20 positions in flatten\n",
    "    with torch.no_grad():\n",
    "        linear_layer.weight.zero_()\n",
    "        out_index = 0\n",
    "        for r_out in range(2):\n",
    "            for c_out in range(3):\n",
    "                out_index = r_out*3 + c_out\n",
    "                # For each kernel cell\n",
    "                for kr in range(3):\n",
    "                    for kc in range(3):\n",
    "                        val = kernel_3x3[kr,kc].item()\n",
    "                        r_in = r_out + kr\n",
    "                        c_in = c_out + kc\n",
    "                        in_index = r_in*5 + c_in  # flatten row-major\n",
    "                        linear_layer.weight[out_index, in_index] = val\n",
    "\n",
    "# Fill it:\n",
    "fill_block_toeplitz_2d(fc2d, conv2d.weight[0,0])\n",
    "print(\"After fill:\", fc2d.weight)\n",
    "\n",
    "# Multiply x_2d flattened\n",
    "x_2d_flat = x_2d.view(1,20)\n",
    "y_fc2d = fc2d(x_2d_flat)  # shape (1,6)\n",
    "y_fc2d_reshaped = y_fc2d.view(2,3)\n",
    "\n",
    "print(\"fc2d output (2x3):\\n\", y_fc2d_reshaped)\n",
    "diff_2d = y_fc2d_reshaped - y_2d[0,0]\n",
    "print(\"Difference vs conv2d:\", diff_2d)\n",
    "\n",
    "# Visual comparison\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(y_2d[0,0].detach(), cmap='viridis')\n",
    "plt.title(\"Conv2D Output\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y_fc2d_reshaped.detach(), cmap='viridis')\n",
    "plt.title(\"Linear Block-Toeplitz Output\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the difference is near zero, we have successfully replicated the 2D convolution with a single large matrix that repeats the 9 kernel parameters in a block-Toeplitz pattern.\n",
    "\n",
    "---\n",
    "## Part 4: Parameter Counts\n",
    "\n",
    "Finally, let’s highlight the massive difference in parameter counts:\n",
    "1. **Full Linear**: (out_features) × (in_features). E.g., 6×20=120 in the 2D case.\n",
    "2. **Block-Toeplitz**: Many repeated entries. Potentially the same matrix shape, but large swaths are repeated. The actual number of **non-zero** unique positions can be computed (9 repeated in different rows for each output patch, i.e. 54 total nonzero, but only 9 unique parameter values if we allow full weight‐sharing). \n",
    "3. **Convolution Kernel**: 3×3=9 parameters.\n",
    "\n",
    "### Student Exercise\n",
    "1. In the 1D case (length=10, output=8), the full linear is 8×10=80. The Toeplitz has 8 rows with 3 nonzero entries each → 24 nonzero. But only 3 actual kernel parameters. \n",
    "2. In the 2D case (20→6), the full linear is 120. The block-Toeplitz might have 6 patches × 9 = 54 nonzero, but only 9 distinct kernel values. \n",
    "\n",
    "Confirm these numbers or fill them in for your assignment.\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "We've now:\n",
    "- Demonstrated **Toeplitz** and **block-Toeplitz** construction for valid convolution in 1D and 2D.\n",
    "- Seen how **padding** changes the dimension and partial equivariance.\n",
    "- Understood that **im2col** is effectively building the block-Toeplitz or patch-based multiplication.\n",
    "- Highlighted the difference between a fully dense linear map and a convolution with **weight sharing**.\n",
    "\n",
    "Feel free to explore different kernels, shift amounts, or pad sizes to deepen understanding.\n",
    "\n",
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "name": "Toeplitz_Padding_Shifts_2D",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

