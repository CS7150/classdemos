{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe811fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import PlotWidget, show, pbar, Widget, Range, Numberbox\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid\n",
    "from torch.nn.functional import mse_loss, cross_entropy\n",
    "from torch.optim import Adam, SGD\n",
    "from collections import OrderedDict\n",
    "import numpy\n",
    "\n",
    "# Checkerboard classificaiton\n",
    "def classify_target(x, y):\n",
    "    return (y.floor() + x.floor()).long() % 2\n",
    "\n",
    "# Sine wave classification\n",
    "def classify_target(x, y):\n",
    "    return (y > (x * 3).sin()).long()\n",
    "\n",
    "# Xor classificaiton\n",
    "def classify_target(x, y):\n",
    "    return (y.sign() != x.sign()).long()\n",
    "\n",
    "#def classify_target(x, y):\n",
    "#    return (x**2 + y**2).long() % 2\n",
    "\n",
    "class TwoDNetworkWidget(Widget):\n",
    "    def __init__(self, classify_target=classify_target):\n",
    "        super().__init__()\n",
    "        self.history = []\n",
    "        self.plot = PlotWidget(self.visualize_net, mosaic='012\\n333', figsize=(11,6),\n",
    "                               bbox_inches='tight', gridspec_kw={'hspace': 0.25, 'height_ratios': [2,1]})\n",
    "        scrubber = Range(min=0, max=0, value=self.plot.prop('index'))\n",
    "        numbox = Numberbox(value=self.plot.prop('index'))\n",
    "        self.content = [\n",
    "            [\n",
    "                [show.style(alignContent='center'), 'Iteration'],\n",
    "                numbox,\n",
    "                show.style(flex=20), scrubber\n",
    "            ],\n",
    "            self.plot\n",
    "        ]\n",
    "        self.plot.on('click', self.plot_click)\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        return show.html(self.content)\n",
    "    \n",
    "    def add(self, net, stats=None):\n",
    "        with torch.no_grad():\n",
    "            self.history.append((deepcopy(net).cpu(), stats))\n",
    "        self.content[0][-1].max = len(self.history) - 1\n",
    "        if len(self.history) == 1:\n",
    "            self.plot.index = len(self.history) - 1\n",
    "\n",
    "    def visualize_net(self, fig, index=0):\n",
    "        def endpoints(w, b, scale=10):\n",
    "            if abs(w[1]) > abs(w[0]):\n",
    "                x0 = torch.tensor([-scale, scale]).to(w.device)\n",
    "                x1 = (-b - w[0] * x0) / w[1]\n",
    "            else:\n",
    "                x1 = torch.tensor([-scale, scale]).to(w.device)\n",
    "                x0 = (-b - w[1] * x1) / w[0]\n",
    "            return torch.stack([x0, x1], dim=1)\n",
    "\n",
    "        ax1, ax2, ax3, ax4 = fig.axes\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax3.clear()\n",
    "        ax4.clear()\n",
    "        if index >= len(self.history):\n",
    "            return\n",
    "        net, data = self.history[index]\n",
    "        grid = torch.stack([\n",
    "            torch.linspace(-2, 2, 100)[None, :].expand(100, 100),\n",
    "            torch.linspace(2, -2, 100)[:, None].expand(100, 100),\n",
    "        ])\n",
    "        x, y = grid\n",
    "        target = classify_target(x, y)\n",
    "        ax1.set_title('target')\n",
    "        ax1.imshow(target.float(), cmap='hot', extent=[-2,2,-2,2], vmin=0, vmax=1)\n",
    "        ax2.set_title('network output')\n",
    "        score = net(grid.permute(1, 2, 0).reshape(-1, 2)).softmax(1)\n",
    "        ax2.imshow(score[:,1].reshape(100, 100).detach().cpu(), cmap='hot', extent=[-2,2,-2,2], vmin=0, vmax=1)\n",
    "\n",
    "        ax3.set_title('first layer folds')\n",
    "        module = [m for m in net.modules() if isinstance(m, torch.nn.Linear)][0]\n",
    "        w = module.weight.detach().cpu()\n",
    "        b = module.bias.detach().cpu()\n",
    "        e = torch.stack([endpoints(wc, bc) for wc, bc in zip(w, b)])\n",
    "        for ep in e:\n",
    "            ax3.plot(ep[:,0], ep[:,1], '#00aa00', linewidth=0.75, alpha=0.33)\n",
    "        ax3.set_ylim(-2, 2)\n",
    "        ax3.set_xlim(-2, 2)\n",
    "        ax3.set_aspect(1.0)\n",
    "        \n",
    "        ax4.set_title('training curve')\n",
    "        ax4.set_xlabel('iteration')\n",
    "        ax4.axvline(index, color='red', linewidth=0.5)\n",
    "        for k, v in data.items():\n",
    "            label = f'{k} = {v:.3g}'\n",
    "            ax4.plot(range(len(self.history)), [h[1][k] for h in self.history], linewidth=0.5, label=label)\n",
    "        ax4.legend()\n",
    "        \n",
    "    def plot_click(self, e):\n",
    "        loc = self.plot.event_location(e)\n",
    "        if loc.axis == 3:\n",
    "            self.plot.index = max(0, min(len(self.history) - 1, int(loc.x + 0.5)))\n",
    "            self.plot.redraw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = TwoDNetworkWidget(classify_target)\n",
    "\n",
    "seed = 5 # is good with Adam\n",
    "seed = 2\n",
    "device='cpu'\n",
    "mlp = torch.nn.Sequential(OrderedDict([\n",
    "    ('layer1', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer2', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer3', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer4', Sequential(Linear(2, 2, bias=False)))\n",
    "]))\n",
    "\n",
    "prng = numpy.random.RandomState(seed)\n",
    "with torch.no_grad():\n",
    "    for p in mlp.parameters():\n",
    "        p[...] = torch.tensor(prng.randn(p.numel())).reshape(p.shape)\n",
    "\n",
    "def sample_x():\n",
    "    return torch.tensor(prng.randn(1000)).float().reshape(500, 2)\n",
    "\n",
    "def softmax(z):\n",
    "    numerator = z.exp()\n",
    "    denominator = numerator.sum(dim=1, keepdim=True)\n",
    "    return numerator / denominator\n",
    "\n",
    "eye2 = torch.eye(2).to(device)\n",
    "def true_probability(x, rule):\n",
    "    classnumber = rule(x[:,0], x[:,1])\n",
    "    return eye2[classnumber]\n",
    "\n",
    "def nce_loss(p, y):\n",
    "    return -(y * p.log()).sum(dim=1).mean(dim=0)\n",
    "\n",
    "def mse_loss(p, y):\n",
    "    return ((p - y)**2).sum(dim=1).mean(dim=0)\n",
    "\n",
    "mlp.to(device)\n",
    "    \n",
    "def my_loss(x):\n",
    "    y = true_probability(x, classify_target)\n",
    "    z = mlp(x)\n",
    "    p = softmax(z)\n",
    "    return mse_loss(p, y)\n",
    "\n",
    "#optimizer = SGD(mlp.parameters(), lr=0.01)\n",
    "optimizer = SGD([\n",
    "    dict(params=mlp.layer1.parameters(), lr=1000.0),\n",
    "    dict(params=mlp.layer2.parameters(), lr=100.0),\n",
    "    dict(params=mlp.layer3.parameters(), lr=1.0),\n",
    "    dict(params=mlp.layer4.parameters(), lr=0.01)\n",
    "], momentum=0.5)\n",
    "optimizer = Adam([\n",
    "    dict(params=mlp.layer1.parameters(), lr=5.0),\n",
    "    dict(params=mlp.layer2.parameters(), lr=0.5),\n",
    "    dict(params=mlp.layer3.parameters(), lr=0.10),\n",
    "    dict(params=mlp.layer4.parameters(), lr=0.01)\n",
    "])\n",
    "for iteration in pbar(range(1000)):\n",
    "    x = sample_x().to(device)\n",
    "    loss = my_loss(x)\n",
    "    grads=[0,0,0,0]\n",
    "    if iteration > 0:\n",
    "        mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        weights = [getattr(mlp, f'layer{i+1}')[0].weight for i in range(4)]\n",
    "        with torch.no_grad():\n",
    "            grads = [w.grad.norm() / w.norm() for w in weights]\n",
    "        optimizer.step()\n",
    "    widget.add(mlp, dict(#loss=loss.detach().item(),\n",
    "                         grad4=grads[3], grad3=grads[2], grad2=grads[1], grad1=grads[0]))\n",
    "widget.plot.index = iteration\n",
    "show(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4541ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "widget = TwoDNetworkWidget(classify_target)\n",
    "\n",
    "seed = 5 # is good with Adam\n",
    "seed = 1\n",
    "device='cpu'\n",
    "mlp = torch.nn.Sequential(OrderedDict([\n",
    "    ('layer1', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer2', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer3', Sequential(Linear(2, 2), Sigmoid())),\n",
    "    ('layer4', Sequential(Linear(2, 2, bias=False)))\n",
    "]))\n",
    "\n",
    "prng = numpy.random.RandomState(seed)\n",
    "with torch.no_grad():\n",
    "    for p in mlp.parameters():\n",
    "        p[...] = torch.tensor(prng.randn(p.numel()) * math.sqrt(5)).reshape(p.shape)\n",
    "\n",
    "def sample_x():\n",
    "    return torch.tensor(prng.randn(1000)).float().reshape(500, 2)\n",
    "\n",
    "def softmax(z):\n",
    "    numerator = z.exp()\n",
    "    denominator = numerator.sum(dim=1, keepdim=True)\n",
    "    return numerator / denominator\n",
    "\n",
    "eye2 = torch.eye(2).to(device)\n",
    "def true_probability(x, rule):\n",
    "    classnumber = rule(x[:,0], x[:,1])\n",
    "    return eye2[classnumber]\n",
    "\n",
    "def nce_loss(p, y):\n",
    "    return -(y * p.log()).sum(dim=1).mean(dim=0)\n",
    "\n",
    "def mse_loss(p, y):\n",
    "    return ((p - y)**2).sum(dim=1).mean(dim=0)\n",
    "\n",
    "mlp.to(device)\n",
    "    \n",
    "def my_loss(x):\n",
    "    y = true_probability(x, classify_target)\n",
    "    z = mlp(x)\n",
    "    p = softmax(z)\n",
    "    return mse_loss(p, y)\n",
    "\n",
    "#optimizer = SGD(mlp.parameters(), lr=0.01)\n",
    "optimizer = Adam([\n",
    "    dict(params=mlp.layer1.parameters(), lr=5.0),\n",
    "    dict(params=mlp.layer2.parameters(), lr=0.5),\n",
    "    dict(params=mlp.layer3.parameters(), lr=0.10),\n",
    "    dict(params=mlp.layer4.parameters(), lr=0.01)\n",
    "])\n",
    "optimizer = SGD(mlp.parameters(), lr=0.01)\n",
    "for iteration in pbar(range(1000)):\n",
    "    x = sample_x().to(device)\n",
    "    loss = my_loss(x)\n",
    "    grads=[0,0,0,0]\n",
    "    if iteration > 0:\n",
    "        mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        weights = [getattr(mlp, f'layer{i+1}')[0].weight for i in range(4)]\n",
    "        with torch.no_grad():\n",
    "            grads = [w.grad.norm() / w.norm() for w in weights]\n",
    "        optimizer.step()\n",
    "    widget.add(mlp, dict(#loss=loss.detach().item(),\n",
    "                         grad4=grads[3], grad3=grads[2], grad2=grads[1], grad1=grads[0]))\n",
    "widget.plot.index = iteration\n",
    "show(widget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
